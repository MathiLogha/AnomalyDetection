{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathiLogha/AnomalyDetection/blob/main/AnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSZ2yPhD10P0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "\n",
        "# from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoOsxZ8g9AS2",
        "outputId": "35315a5c-a6b4-4133-8de3-6ac684d2d749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d odins0n/ucf-crime-dataset\n",
        "!unzip ucf-crime-dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8m4jr5v9C90",
        "outputId": "aaa60dc7-9fda-470d-8c28-72c4acc5f031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "unzip:  cannot find or open ucf-crime-dataset.zip, ucf-crime-dataset.zip.zip or ucf-crime-dataset.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_dir = \"ufc-crime-dataset/Train\"\n",
        "test_dir = \"ufc-crime-dataset/Test\"\n",
        "\n",
        "print(os.listdir(train_dir))\n",
        "print(os.listdir(test_dir))"
      ],
      "metadata": {
        "id": "bIAaZvLR9GTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTDfhmXNF0Yd"
      },
      "outputs": [],
      "source": [
        "# Load the train and test data\n",
        "X_train, y_train = load_data(train_dir)\n",
        "X_test, y_test = load_data(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 12\n",
        "IMG_HEIGHT = 64\n",
        "IMG_WIDTH = 64\n",
        "BATCH_SIZE = 46      #Was 46\n",
        "EPOCHS = 1\n",
        "LR =  0.00003\n",
        "NUM_CLASSES = 14\n",
        "CLASS_LABELS = ['Abuse','Arrest','Arson','Assault','Burglary','Explosion','Fighting',\"Normal\",'RoadAccidents','Robbery','Shooting','Shoplifting','Stealing','Vandalism']"
      ],
      "metadata": {
        "id": "6eAKZwdk-9Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_fun = tf.keras.applications.densenet.preprocess_input\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "                                   #horizontal_flip=True,\n",
        "                                   #width_shift_range=0.1,\n",
        "                                   #height_shift_range=0.05,\n",
        "                                   rescale = 1./255,\n",
        "                                   validation_split=0.2, # set validation split, Added\n",
        "                                   preprocessing_function=preprocess_fun\n",
        "                                  )\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                  preprocessing_function=preprocess_fun\n",
        "                                 )"
      ],
      "metadata": {
        "id": "JDemlMpS_BQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(directory = train_dir,\n",
        "                                                    target_size = (IMG_HEIGHT ,IMG_WIDTH),\n",
        "                                                    batch_size = BATCH_SIZE,\n",
        "                                                    shuffle  = True ,\n",
        "                                                    color_mode = \"rgb\",\n",
        "                                                    class_mode = \"categorical\",\n",
        "                                                     subset='training',    #Added\n",
        "                                                    seed = SEED\n",
        "                                                   )\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(train_dir, # same directory as training data\n",
        "                                                    target_size = (IMG_HEIGHT ,IMG_WIDTH),\n",
        "                                                    batch_size = BATCH_SIZE,\n",
        "                                                    shuffle  = True ,\n",
        "                                                    color_mode = \"rgb\",\n",
        "                                                    class_mode = \"categorical\",\n",
        "                                                    subset='validation', # set as validation data,  #Added\n",
        "                                                    seed = SEED\n",
        "                                                   )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(directory = test_dir,\n",
        "                                                   target_size = (IMG_HEIGHT ,IMG_WIDTH),\n",
        "                                                    batch_size = BATCH_SIZE,\n",
        "                                                    shuffle  = False ,\n",
        "                                                    color_mode = \"rgb\",\n",
        "                                                    class_mode = \"categorical\",\n",
        "                                                    seed = SEED\n",
        "                                                  )"
      ],
      "metadata": {
        "id": "n9adPiiL_HEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(x = CLASS_LABELS,\n",
        "             y = [list(train_generator.classes).count(i) for i in np.unique(train_generator.classes)] ,\n",
        "             color = np.unique(train_generator.classes) ,\n",
        "             color_continuous_scale=\"Emrld\")\n",
        "fig.update_xaxes(title=\"Classes\")\n",
        "fig.update_yaxes(title = \"Number of Images\")\n",
        "fig.update_layout(showlegend = True,\n",
        "    title = {\n",
        "        'text': 'Train Data Distribution ',\n",
        "        'y':0.95,\n",
        "        'x':0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "DVyFUXuK_NTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(x = CLASS_LABELS,\n",
        "             y = [list(validation_generator.classes).count(i) for i in np.unique(validation_generator.classes)] ,\n",
        "             color = np.unique(train_generator.classes) ,\n",
        "             color_continuous_scale=\"Emrld\")\n",
        "fig.update_xaxes(title=\"Classes\")\n",
        "fig.update_yaxes(title = \"Number of Images\")\n",
        "fig.update_layout(showlegend = True,\n",
        "    title = {\n",
        "        'text': 'Validation Data Distribution ',\n",
        "        'y':0.95,\n",
        "        'x':0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hUA8P3ZE_PT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(x = CLASS_LABELS,\n",
        "             y = [list(test_generator.classes).count(i) for i in np.unique(test_generator.classes)] ,\n",
        "             color = np.unique(train_generator.classes) ,\n",
        "             color_continuous_scale=\"Emrld\")\n",
        "fig.update_xaxes(title=\"Classes\")\n",
        "fig.update_yaxes(title = \"Number of Images\")\n",
        "fig.update_layout(showlegend = True,\n",
        "    title = {\n",
        "        'text': 'Test Data Distribution ',\n",
        "        'y':0.95,\n",
        "        'x':0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "jxenyGG0_SUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extractor(inputs):\n",
        "    feature_extractor = tf.keras.applications.DenseNet121(input_shape=(IMG_HEIGHT,IMG_WIDTH, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights=\"imagenet\")(inputs)\n",
        "\n",
        "    return feature_extractor\n",
        "\n",
        "def classifier(inputs):\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.4) (x)\n",
        "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"classification\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def final_model(inputs):\n",
        "    densenet_feature_extractor = feature_extractor(inputs)\n",
        "    classification_output = classifier(densenet_feature_extractor)\n",
        "\n",
        "    return classification_output\n",
        "\n",
        "def define_compile_model():\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=(IMG_HEIGHT ,IMG_WIDTH,3))\n",
        "    classification_output = final_model(inputs)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs = classification_output)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(LR),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics = [tf.keras.metrics.AUC()])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = define_compile_model()\n",
        "# clear_output()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "xaOgPeDX_Vmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x = train_generator,validation_data=validation_generator,epochs = EPOCHS)\n",
        "#history = model.fit(x = train_generator,validation_split=0.2,epochs = EPOCHS)"
      ],
      "metadata": {
        "id": "UJNv5qfJ_Xzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_generator)\n",
        "y_test = test_generator.classes\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (15,8))\n",
        "\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    for (idx, c_label) in enumerate(CLASS_LABELS):\n",
        "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
        "        c_ax.plot(fpr, tpr,lw=2, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
        "    c_ax.plot(fpr, fpr, 'black',linestyle='dashed', lw=4, label = 'Random Guessing')\n",
        "    return roc_auc_score(y_test, y_pred, average=average)\n",
        "\n",
        "print('ROC AUC score:', multiclass_roc_auc_score(y_test , preds  , average = \"micro\"))\n",
        "plt.xlabel('FALSE POSITIVE RATE', fontsize=18)\n",
        "plt.ylabel('TRUE POSITIVE RATE', fontsize=16)\n",
        "plt.legend(fontsize = 11.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DO2ZuzPP_Yso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2sWaphTdak_"
      },
      "outputs": [],
      "source": [
        "# Set the dimensions of the input images\n",
        "img_width = 112\n",
        "img_height = 112\n",
        "img_depth = 32\n",
        "\n",
        "# Set the number of classes\n",
        "num_classes = 2\n",
        "\n",
        "# Set the batch size and number of epochs\n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "\n",
        "# Define the C3D model\n",
        "def c3d_model(input_shape, num_classes):\n",
        "    inputs = Input(input_shape)\n",
        "    x = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling3D(pool_size=(2, 2, 1), strides=(2, 2, 1))(x)\n",
        "    x = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)\n",
        "    x = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(x)\n",
        "    x = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)\n",
        "    x = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(x)\n",
        "    x = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(4096, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(4096, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Define the MIL ranking loss function\n",
        "def ranking_loss(y_true, y_pred):\n",
        "    num_segments = K.int_shape(y_pred)[0]\n",
        "    y_pred = K.reshape(y_pred, (num_segments, -1))\n",
        "    y_true = K.cast(y_true, 'int32')\n",
        "    y_true = K.one_hot(y_true, num_classes)\n",
        "    y_true = K.reshape(y_true, (num_segments, -1))\n",
        "    pos = K.sum(y_true * y_pred, axis=-1)\n",
        "    neg = K.sum((1 - y_true) * y_pred, axis=-1)\n",
        "    loss = K.maximum(0.0, 1 - pos + neg)\n",
        "    return K.mean(loss)\n",
        "\n",
        "# Define the data generators for the train and test sets\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Build the C3D feature extraction model\n",
        "c3d_model = Sequential()\n",
        "\n",
        "# Add the first convolutional layer\n",
        "c3d_model.add(Conv3D(64, kernel_size=(3, 3, 3), input_shape=input_shape, padding='same'))\n",
        "c3d_model.add(Activation('relu'))\n",
        "c3d_model.add(MaxPooling3D(pool_size=(2, 2, 1), strides=(2, 2, 1), padding='valid'))\n",
        "\n",
        "# Add the second convolutional layer\n",
        "c3d_model.add(Conv3D(128, kernel_size=(3, 3, 3), padding='same'))\n",
        "c3d_model.add(Activation('relu'))\n",
        "c3d_model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid'))\n",
        "\n",
        "# Add the third convolutional layer\n",
        "c3d_model.add(Conv3D(256, kernel_size=(3, 3, 3), padding='same'))\n",
        "c3d_model.add(Activation('relu'))\n",
        "c3d_model.add(Conv3D(256, kernel_size=(3, 3, 3), padding='same'))\n",
        "c3d_model\n",
        "\n",
        "\n",
        "\n",
        "# Create a sequential model for the fully connected neural network\n",
        "fcn_model = Sequential()\n",
        "fcn_model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "fcn_model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "fcn_model.add(MaxPooling1D(pool_size=2))\n",
        "fcn_model.add(Flatten())\n",
        "fcn_model.add(Dense(128, activation='relu'))\n",
        "fcn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the fully connected neural network model\n",
        "fcn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Set the MIL ranking loss function and compile the model\n",
        "model.compile(loss=mil_loss, optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, validation_data=val_generator, epochs=10)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLvONXgHuV94P64n/bGt1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}